Ground-up implementation of Baum-Welch - Convergence and accuracy issues.

Applying it to the target problem, dividing the domain in X-direction, into discrete steps,
that represent the states in the Baum Welch

Checking if the Baum-Welch version that supported the observations to be continuous random variables, could 
be employed for this particular problem.
====>>> [ DESCRIBED BELOW ] <<<====

Problems with Ground-up impl, switch to library.
Python Library gave problems wrt convergence and stability

Visualization of the available data in form of labels, various histograms of problem parameters.

Visualization of the domain in which the robot moved, available in form of labels. [ The nearly circular pattern ]

Modeling HMM states in a different way.

Using MATLAB builtin libs for getting HMM done.

Use of Viterbi to map states to the parameters in the problem.

For Prediction of final position... Use of techniques like:	 [ The choices made need to be justified ]

		Assuming a constant rate of change for the value of one of the hidden variable (angular velocity)
		and finding the region of intersection of the ray with the circular region where the bot hops.
		{{ This mostly seems like a non ML approach, but I might be missing some details }}

	OR

			use of E and T matrices to predict the State and Observation for timestep 1001
			====>>> [ DESCRIBED BELOW ] <<<====

			AND

				computing the median of all labelled data-points belonging to a State,
				such that they also had the exact same value for the observed variable.

				OR (when the above option wasn't feasible)

				considering the mean/median of the labelled data-points belonging to a State,
				and computing the projection of that mean onto the ray, to find the 1001 bot location.


[ 8< --------------------------------------------------------------------------------------------------------------- >8 ]


	{	A description of why the idea of using a HMM with continuous Observed Variables seemed convinving,	}
	{	and why it didn't seem feasible to have it implemented and applied to the target problem.			}

While HMM seemed to be the most appropriate way of modelling the problem, one essential difference that existed
between the target problem and the typical HMM use cases, was that, the the hidden as well as the observed random 
variables were of continuous nature in the bot problem, unlike the discrete nature in case of other HMM applications.
Obviously discretizing the continuous domain was the approach taken.
But this is subject to the error that is introduced because of the quantization, and consequently larger 
quantization steps give worse accuracy.
We searched if there has been prior work done for developing HMM variants that deal with continuous random variables.
We found one paper that described HMMs with continuous-time, and another one that described HMMs with the observations
as continuous random variables.
The latter seemed better suited for the target problem since the time-steps are discrete and the observations continuous.
The approach models observation probabilities as continuous density functions, generally a Gaussian.
The Emission probabilities get expressed in terms of the parameters of the distribution.
These parameters are the mixture coefficient, the mean and the covariance; which are learnt during the E-M process.
Although the approach seemed convincing from the point of view of the accuracy, it was evident that this would involve
an increased amount of computation as compared to the discrete HMM version, since there are 3 parameters to be learnt
instead of the single entry in the Emission table cells. Also the number of floating point operations would increase,
for dealing with the probability density functions.
Given that we were facing a challenge with the running time and numerical stability of the the discrete version itself,
we de-prioritized the actual implementation of the continuous HMM and considered it as an option to look out for increasing
accuracy once the discrete model gave satisfactory results.


[ 8< --------------------------------------------------------------------------------------------------------------- >8 ]


	{ use of E and T matrices to predict the State and Observation for timestep 1001 }

In order to predict the position of the bot at the 1001th time-step, it is essential to know the State and the value
of the observed variable at time-step 1001.
The approach we took for deriving this, was to utilize the Emission and Transition probability tables that were
generated by the E-M based training, and the mapping of bot-position to States generated by the Viterbi method.
The State at time-step 1000 was known from the outcome of the Viterbi algorithm,
The row in Transition probability matrix corresponding this State was scanned to locate the most likely 'next-State'.
This was the State for which the Transition probability from S(t=1000) was maximum.
Once S(t=1001) was identified, the Emission probability matrix was looked up to identify the most likely value of the 
observed variable corresponding to that state.
The process of doing this was similar; scan the row in the Emission probability matrix corresponding to S(t=1001),
to locate the observation value that had highest emission probability.

Once, the most likely value of the observed variable at time-step 1001 was deduced, the following method was used
for computing the cartesian coordinates of the bot's location. ...


